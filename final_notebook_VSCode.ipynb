{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd899e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from mpl_toolkits import mplot3d\n",
    "import seaborn as sns  # requires version 0.10.1/nur 0.11.1 funktioniert\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import scipy.io\n",
    "import scipy.stats as st\n",
    "import mat73\n",
    "import time\n",
    "import pickle5 as pickle\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# to disable seaborn warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Datensatz\n",
    "anz_samples = 200000  \n",
    "\n",
    "comp_samples = 5000 # anz_samples + 4*comp_samples must < 1.000.000\n",
    "\n",
    "# Batchsize\n",
    "batch_size_train = 1024 \n",
    "batch_size_test = 4096\n",
    "\n",
    "# Training\n",
    "n_epochs = 5000\n",
    "learning_rate = 0.01   \n",
    "used_optimizer = \"adagrad\" # (adam=Adam, nadam=NAdam, radam=RAdam, adamax=Adamax, adagrad=Adagrad, sgd=SGD, adadelta=Adadelta, rms=RMSProp) \n",
    "used_loss = \"mse\" # (mse=mean squared error, mae=mean average error, kl=kullback leibler)\n",
    "\n",
    "used_init = \"he\" # (he=Kaiming He, xavier=Xavier)\n",
    "used_activation = \"leaky_relu\" # (leaky_relu=Leaky ReLU, relu=ReLU, elu=ELU, gelu=GELU, tanh=tanh)\n",
    "\n",
    "use_dropout = [True, 0.6] # (True=dropout, 0.1=wahrscheinlichkeit ein Neuron zu löschen)\n",
    "use_weight_decay = [False, 0.0] # pytorch weight decay\n",
    "use_noise = [False, 0.0, 0.005] # (True=noise, mean, stddev)\n",
    "\n",
    "# log\n",
    "log_interval = 500\n",
    "use_plot = True\n",
    "\n",
    "# Neuronales Netz\n",
    "I1 = 238           # Anzahl Input Parameter\n",
    "L1 = 1024          # Neuronen Layer1\n",
    "L2 = 1024          # Neuronen Layer2 \n",
    "L3 = 1024          # Neuronen Layer3 \n",
    "L4 = 2048          # Neuronen Layer4 \n",
    "L5 = 2048          # Neuronen Layer5 \n",
    "O1 = 3600          # Anzahl Output Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b74d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "\n",
    "def load_data():\n",
    "    infile = open(r\"path/to/file.mat\",'rb')\n",
    "    data = pickle.load(infile)\n",
    "    infile.close()\n",
    "    \n",
    "    samples_pro_verteilung = int(anz_samples/4)\n",
    "    \n",
    "    x = []\n",
    "    x.extend(data[\"input\"][     0: samples_pro_verteilung         ])\n",
    "    x.extend(data[\"input\"][250000: samples_pro_verteilung + 250000])\n",
    "    x.extend(data[\"input\"][500000: samples_pro_verteilung + 500000])\n",
    "    x.extend(data[\"input\"][750000: samples_pro_verteilung + 750000])\n",
    "\n",
    "    y = []\n",
    "    y.extend(data[\"ref\"][     0: samples_pro_verteilung         ])\n",
    "    y.extend(data[\"ref\"][250000: samples_pro_verteilung + 250000])\n",
    "    y.extend(data[\"ref\"][500000: samples_pro_verteilung + 500000])\n",
    "    y.extend(data[\"ref\"][750000: samples_pro_verteilung + 750000])\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    x_test_comp = []\n",
    "    x_test_comp.append(data[\"input\"][         samples_pro_verteilung :\n",
    "                                                    samples_pro_verteilung + comp_samples])\n",
    "    x_test_comp.append(data[\"input\"][250000 + samples_pro_verteilung :\n",
    "                                                    250000 + samples_pro_verteilung + comp_samples])\n",
    "    x_test_comp.append(data[\"input\"][500000 + samples_pro_verteilung :\n",
    "                                                    500000 + samples_pro_verteilung + comp_samples])\n",
    "    x_test_comp.append(data[\"input\"][750000 + samples_pro_verteilung :\n",
    "                                                    750000 + samples_pro_verteilung + comp_samples])\n",
    "    \n",
    "    y_test_comp = []\n",
    "    y_test_comp.append(data[\"ref\"][         samples_pro_verteilung :\n",
    "                                            samples_pro_verteilung + comp_samples])\n",
    "    y_test_comp.append(data[\"ref\"][250000 + samples_pro_verteilung :\n",
    "                                            250000 + samples_pro_verteilung + comp_samples])\n",
    "    y_test_comp.append(data[\"ref\"][500000 + samples_pro_verteilung :\n",
    "                                            500000 + samples_pro_verteilung + comp_samples])\n",
    "    y_test_comp.append(data[\"ref\"][750000 + samples_pro_verteilung :\n",
    "                                            750000 + samples_pro_verteilung + comp_samples])\n",
    "    \n",
    "    x_test_comp = np.array(x_test_comp)\n",
    "    y_test_comp = np.array(y_test_comp)\n",
    "        \n",
    "    # Split into training and test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, shuffle=True) \n",
    "\n",
    "    # transforming training and test sets to torch.tensor\n",
    "    x_train = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
    "    y_train = torch.from_numpy(y_train).type(torch.FloatTensor)\n",
    "    \n",
    "    x_test = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
    "    y_test = torch.from_numpy(y_test).type(torch.FloatTensor)\n",
    "\n",
    "    x_test_comp = torch.from_numpy(x_test_comp).type(torch.FloatTensor)\n",
    "    y_test_comp = torch.from_numpy(y_test_comp).type(torch.FloatTensor)\n",
    "\n",
    "    device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Get Batches via DataLoader\n",
    "    train = torch.utils.data.TensorDataset(x_train.to(device), y_train.to(device))\n",
    "    test = torch.utils.data.TensorDataset(x_test.to(device), y_test.to(device))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=batch_size_train, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=batch_size_test, shuffle=True)\n",
    "    \n",
    "    return train_loader, test_loader, x_train, y_train, x_test, y_test, x_test_comp, y_test_comp\n",
    "\n",
    "#Definition of the net structure\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        if used_init == 'he':\n",
    "            self.fc1 = nn.Linear(I1, L1) # 238 eingangsparameter\n",
    "            torch.nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in') \n",
    "            self.fc2 = nn.Linear(L1, L2)\n",
    "            torch.nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in')  \n",
    "            self.fc3 = nn.Linear(L2, L3)\n",
    "            torch.nn.init.kaiming_normal_(self.fc3.weight, mode='fan_in') \n",
    "            self.fc4 = nn.Linear(L3, L4)\n",
    "            torch.nn.init.kaiming_normal_(self.fc4.weight, mode='fan_in') \n",
    "            self.fc5 = nn.Linear(L4, L5)\n",
    "            torch.nn.init.kaiming_normal_(self.fc5.weight, mode='fan_in') \n",
    "            self.fc6 = nn.Linear(L5, O1) # 3600 ausgaben zu schätzen\n",
    "        \n",
    "        elif used_init == 'xavier':\n",
    "            self.fc1 = nn.Linear(I1, L1) # 238 eingangsparameter\n",
    "            torch.nn.init.xavier_uniform_(self.fc1.weight) \n",
    "            self.fc2 = nn.Linear(L1, L2)\n",
    "            torch.nn.init.xavier_uniform_(self.fc2.weight) \n",
    "            self.fc3 = nn.Linear(L2, L3)\n",
    "            torch.nn.init.xavier_uniform_(self.fc3.weight) \n",
    "            self.fc4 = nn.Linear(L3, L4)\n",
    "            torch.nn.init.xavier_uniform_(self.fc4.weight) \n",
    "            self.fc5 = nn.Linear(L4, L5)\n",
    "            torch.nn.init.xavier_uniform_(self.fc5.weight) \n",
    "            self.fc6 = nn.Linear(L5, O1) # 3600 ausgaben zu schätzen\n",
    "    \n",
    "    def forward(self, x):     \n",
    "        if use_dropout[0] == True:\n",
    "            if used_activation == \"leaky_relu\":\n",
    "                x = F.leaky_relu(self.fc1(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.leaky_relu(self.fc2(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.leaky_relu(self.fc3(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.leaky_relu(self.fc4(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.leaky_relu(self.fc5(x))\n",
    "                # x = F.dropout(x, p = use_dropout[1]) # deactivate dropout in last layer \n",
    "                x = self.fc6(x)\n",
    "                return F.softmax(x, dim=-1) \n",
    "            \n",
    "            elif used_activation == \"relu\":\n",
    "                x = F.relu(self.fc1(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.relu(self.fc2(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.relu(self.fc3(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.relu(self.fc4(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.relu(self.fc5(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = self.fc6(x)\n",
    "                return F.softmax(x, dim=-1)\n",
    "            \n",
    "            elif used_activation == \"elu\":\n",
    "                x = F.elu(self.fc1(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.elu(self.fc2(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.elu(self.fc3(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.elu(self.fc4(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.elu(self.fc5(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = self.fc6(x)\n",
    "                return F.softmax(x, dim=-1) \n",
    "            \n",
    "            elif used_activation == \"gelu\":\n",
    "                x = F.gelu(self.fc1(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.gelu(self.fc2(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.gelu(self.fc3(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.gelu(self.fc4(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.gelu(self.fc5(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = self.fc6(x)\n",
    "                return F.softmax(x, dim=-1) \n",
    "            \n",
    "            elif used_activation == \"tanh\":\n",
    "                x = F.tanh(self.fc1(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.tanh(self.fc2(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.tanh(self.fc3(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.tanh(self.fc4(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = F.tanh(self.fc5(x))\n",
    "                x = F.dropout(x, p = use_dropout[1])\n",
    "                x = self.fc6(x)\n",
    "                return F.softmax(x, dim=-1) \n",
    "\n",
    "        \n",
    "        elif use_dropout[0] == False:\n",
    "            if used_activation == \"leaky_relu\":\n",
    "                x = F.leaky_relu(self.fc1(x))\n",
    "                x = F.leaky_relu(self.fc2(x))\n",
    "                x = F.leaky_relu(self.fc3(x))\n",
    "                x = F.leaky_relu(self.fc4(x))\n",
    "                x = F.leaky_relu(self.fc5(x))\n",
    "                x = self.fc6(x)\n",
    "                return F.softmax(x, dim=-1) \n",
    "            \n",
    "            elif used_activation == \"relu\":\n",
    "                x = F.relu(self.fc1(x))\n",
    "                x = F.relu(self.fc2(x))\n",
    "                x = F.relu(self.fc3(x))\n",
    "                x = F.relu(self.fc4(x))\n",
    "                x = F.relu(self.fc5(x))\n",
    "                x = self.fc6(x)\n",
    "                return F.softmax(x, dim=-1) \n",
    "            \n",
    "            elif used_activation == \"elu\":\n",
    "                x = F.elu(self.fc1(x))\n",
    "                x = F.elu(self.fc2(x))\n",
    "                x = F.elu(self.fc3(x))\n",
    "                x = F.elu(self.fc4(x))\n",
    "                x = F.elu(self.fc5(x))\n",
    "                x = self.fc6(x)\n",
    "                return F.softmax(x, dim=-1)\n",
    "            \n",
    "            elif used_activation == \"gelu\":\n",
    "                x = F.gelu(self.fc1(x))\n",
    "                x = F.gelu(self.fc2(x))\n",
    "                x = F.gelu(self.fc3(x))\n",
    "                x = F.gelu(self.fc4(x))\n",
    "                x = F.gelu(self.fc5(x))\n",
    "                x = self.fc6(x)\n",
    "                return F.softmax(x, dim=-1) \n",
    "            \n",
    "            elif used_activation == \"tanh\":\n",
    "                x = F.tanh(self.fc1(x))\n",
    "                x = F.tanh(self.fc2(x))\n",
    "                x = F.tanh(self.fc3(x))\n",
    "                x = F.tanh(self.fc4(x))\n",
    "                x = F.tanh(self.fc5(x))\n",
    "                x = self.fc6(x)\n",
    "                return F.softmax(x, dim=-1)\n",
    "\n",
    "def preprocess():\n",
    "    # Get cpu or gpu device for training.\n",
    "    device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # print(f\"Using {device} device\")\n",
    "    \n",
    "    network = Net().to(device) # load network to gpu\n",
    "    \n",
    "    if use_weight_decay == True:\n",
    "        if used_optimizer == \"nadam\":\n",
    "            optimizer = optim.NAdam(network.parameters(), lr=learning_rate, weight_decay=use_weight_decay[1])\n",
    "        elif used_optimizer == \"adam\":\n",
    "            optimizer = optim.Adam(network.parameters(), lr=learning_rate, weight_decay=use_weight_decay[1])\n",
    "        elif used_optimizer == \"sgd\":\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate, weight_decay=use_weight_decay[1])\n",
    "        elif used_optimizer == \"radam\":\n",
    "            optimizer = optim.RAdam(network.parameters(), lr=learning_rate, weight_decay=use_weight_decay[1])\n",
    "        elif used_optimizer == \"adamax\":\n",
    "            optimizer = optim.Adamax(network.parameters(), lr=learning_rate, weight_decay=use_weight_decay[1])\n",
    "        elif used_optimizer == \"adadelta\":\n",
    "            optimizer = optim.Adadelta(network.parameters(), lr=learning_rate, weight_decay=use_weight_decay[1])\n",
    "        elif used_optimizer == \"rms\":\n",
    "            optimizer = optim.RMSprop(network.parameters(), lr=learning_rate, weight_decay=use_weight_decay[1])\n",
    "        elif used_optimizer == \"adagrad\":\n",
    "            optimizer = optim.Adagrad(network.parameters(), lr=learning_rate, weight_decay=use_weight_decay[1])\n",
    "\n",
    "    elif use_weight_decay == False:\n",
    "        if used_optimizer == \"nadam\":\n",
    "            optimizer = optim.NAdam(network.parameters(), lr=learning_rate)\n",
    "        elif used_optimizer == \"adam\":\n",
    "            optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "        elif used_optimizer == \"sgd\":\n",
    "            optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
    "        elif used_optimizer == \"radam\":\n",
    "            optimizer = optim.RAdam(network.parameters(), lr=learning_rate)\n",
    "        elif used_optimizer == \"adamax\":\n",
    "            optimizer = optim.Adamax(network.parameters(), lr=learning_rate)\n",
    "        elif used_optimizer == \"adadelta\":\n",
    "            optimizer = optim.Adadelta(network.parameters(), lr=learning_rate)\n",
    "        elif used_optimizer == \"rms\":\n",
    "            optimizer = optim.RMSprop(network.parameters(), lr=learning_rate)\n",
    "        elif used_optimizer == \"adagrad\":\n",
    "            optimizer = optim.Adagrad(network.parameters(), lr=learning_rate)\n",
    "\n",
    "    return network, optimizer\n",
    "\n",
    "def gaussian(ins, is_training=True, mean=0.0, stddev=0.005):\n",
    "    if is_training:\n",
    "        noise = Variable(ins.data.new(ins.size()).normal_(mean, stddev))\n",
    "        return ins + noise\n",
    "    return ins\n",
    "    \n",
    "def predict(network, data):\n",
    "    y_pred = network(data)\n",
    "    return y_pred\n",
    "\n",
    "# Training function for each epoch\n",
    "def train(epoch, network, optimizer):\n",
    "    \n",
    "    network.train()\n",
    "    \n",
    "    # Loop over the batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        target = target.to(torch.float32)\n",
    "        data = data.to(torch.float32)\n",
    "\n",
    "        if use_noise[0] == True:\n",
    "            data = gaussian(data, is_training=True, mean=use_noise[1], stddev=use_noise[2])\n",
    "\n",
    "        # --- Steps of the training of the net ---\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "\n",
    "        if used_loss == \"mse\":\n",
    "            loss = F.mse_loss(output, target) \n",
    "        elif used_loss == \"mae\":\n",
    "            loss = nn.L1Loss(output, target)\n",
    "        elif used_loss == \"kl\":\n",
    "            kl_loss = nn.KLDivLoss(reduction = 'batchmean')\n",
    "            loss = kl_loss(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- Save Evaluation metrics ---\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    train_losses.append(sum(train_loss) / len(train_loader))\n",
    "    train_loss.clear()\n",
    "\n",
    "# Test function that applies the test set to the trained net\n",
    "def test(network):\n",
    "    \n",
    "    network.eval()\n",
    "    \n",
    "    # Gradient calculation is disabled (as not needed)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Loop over the batches\n",
    "        for data, target in test_loader:\n",
    "\n",
    "            target = target.to(torch.float32)\n",
    "            data = data.to(torch.float32)\n",
    "\n",
    "            # --- Prediction and calculation of evaluation metrics ---\n",
    "            output = network(data) \n",
    "\n",
    "            if used_loss == \"mse\":\n",
    "                loss = F.mse_loss(output, target) \n",
    "            elif used_loss == \"mae\":\n",
    "                loss = nn.L1Loss(output, target)\n",
    "            elif used_loss == \"kl\":\n",
    "                kl_loss = nn.KLDivLoss(reduction = 'batchmean')\n",
    "                loss = kl_loss(output, target)\n",
    "\n",
    "            test_loss.append(loss.item())\n",
    "\n",
    "        test_losses.append(sum(test_loss) / len(test_loader))\n",
    "        test_loss.clear()\n",
    "\n",
    "def train_test(network, optimizer):\n",
    "    start_time = time.time() # Start timer\n",
    "\n",
    "    # Initial Evaluation metrics\n",
    "    test(network)\n",
    "    print('Test set: Avg. loss: {:.10f},\\n'.format(test_losses[-1], len(test_loader.dataset)))\n",
    "        \n",
    "    # run training\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train(epoch, network, optimizer)\n",
    "        test(network)\n",
    "\n",
    "        # print evaluation metrics\n",
    "        if epoch % log_interval == 0:\n",
    "           print(\"Epoch \", epoch)\n",
    "           print('Train set: Avg. loss: {:.10f}'.format(train_losses[-1], len(train_loader.dataset)))\n",
    "           print('Test set: Avg. loss: {:.10f}'.format(test_losses[-1], len(test_loader.dataset)))\n",
    "        \n",
    "           passed_sec_epoch = time.time() - start_time\n",
    "           print('geschätzte verbleibende Dauer nach', (passed_sec_epoch/3600), ':', ((((passed_sec_epoch/epoch)*n_epochs)/3600)-(passed_sec_epoch/3600)), '\\n') # auf aktuellem tempo geschätze gesamtdauer-bisherige dauer \n",
    "\n",
    "    passed_sec = time.time() - start_time\n",
    "    passed_min = passed_sec/60\n",
    "    passed_hrs = passed_min/60\n",
    "    print(\"--- %s seconds ---\" % (passed_sec))\n",
    "    print(\"--- %s minutes ---\" % (passed_min))\n",
    "    print(\"--- %s hours ---\" % (passed_hrs))\n",
    "    \n",
    "    return network\n",
    "\n",
    "# Test function that applies the test set to the trained net\n",
    "def test_comp(network, x_data_comp, y_data_comp):\n",
    "    test_loss_comp = []\n",
    "    test_losses_comp = []\n",
    "    \n",
    "    device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    test_data_comp = torch.utils.data.TensorDataset(x_data_comp.to(device), y_data_comp.to(device))\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_data_comp, batch_size=comp_samples, shuffle=False) # shuffle?\n",
    "    \n",
    "    network.eval()\n",
    "    # Gradient calculation is disabled (as not needed)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Loop over the batches\n",
    "        for data, target in test_loader:\n",
    "\n",
    "            target = target.to(torch.float32)\n",
    "            data = data.to(torch.float32)\n",
    "\n",
    "            # --- Prediction and calculation of evaluation metrics ---\n",
    "            output = network(data)\n",
    "\n",
    "            if used_loss == \"mse\":\n",
    "                loss = F.mse_loss(output, target) \n",
    "            elif used_loss == \"mae\":\n",
    "                loss = nn.L1Loss(output, target)\n",
    "            elif used_loss == \"kl\":\n",
    "                kl_loss = nn.KLDivLoss(reduction = 'batchmean')\n",
    "                loss = kl_loss(output, target)\n",
    "\n",
    "            test_loss_comp.append(loss.item())\n",
    "\n",
    "        test_losses_comp.append(sum(test_loss_comp) / len(test_loader))\n",
    "        test_loss_comp.clear()\n",
    "        \n",
    "        return sum(test_losses_comp)/len(test_losses_comp)\n",
    "        \n",
    "def plot_train_test_loss(epochen, train_losses, test_losses):  \n",
    "    fig = plt.figure()\n",
    "    plt.plot(epochen, train_losses,  color='blue',  label='Train Loss')\n",
    "    plt.plot(epochen, test_losses[1:],  color='red', label='Test Loss')\n",
    "    plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "    plt.xlabel('number of training examples seen')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "  \n",
    "def plot_3d_spect_vergleich(gt_data, nn_data, winkel1=90, winkel2=0):\n",
    "    # gt_data: y_train daten, Ground Truth\n",
    "    # nn_data: vorhersage des NN\n",
    "    # winkel1: Winkel zur Grundfläche\n",
    "    # winkel2: Rotation der x-y achsen\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    x = np.linspace(0, 60, 60)\n",
    "    y = np.linspace(0, 60, 60)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = gt_data.reshape(60, 60) # Ground Truth\n",
    "    \n",
    "    axes0 = fig.add_subplot(121, projection='3d')\n",
    "    axes0.view_init(winkel1, winkel2) # erste Zahl Winkel zur Grundfläche, zweite Zahl rotation der x-y achsen\n",
    "    axes0.plot_surface(X, Y, Z, rstride=1, cstride=1,cmap='viridis', edgecolor='none')\n",
    "    axes0.set_title(\"realer Datenpunkt\");\n",
    "    \n",
    "    Z = nn_data.reshape(60, 60) \n",
    "    axes1 = fig.add_subplot(122, projection='3d')\n",
    "    axes1.contour3D(X, Y, Z, 50, cmap='binary')\n",
    "    axes1.view_init(winkel1, winkel2) # erste Zahl Winkel zur Grundfläche, zweite Zahl rotation der x-y achsen\n",
    "    axes1.plot_surface(X, Y, Z, rstride=1, cstride=1,cmap='viridis', edgecolor='none')\n",
    "    axes1.set_title(\"geschätzte Verteilung\");\n",
    "    \n",
    "def dice_loss(inputs, targets, smooth=1):\n",
    "    # flatten label and prediction tensors\n",
    "    inputs = inputs.reshape(-1) \n",
    "    targets = targets.reshape(-1) \n",
    "\n",
    "    intersection = (inputs * targets).sum()                            \n",
    "    dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "\n",
    "    return dice\n",
    "    \n",
    "def gmm_eval(network, seg_mask, n_comp=4):\n",
    "    # seg_mask = 'brain_tissue', 'wm', 'gm', 'csf'\n",
    "    \n",
    "    # load invivo daten\n",
    "    invivo_data = scipy.io.loadmat(r\"path/to/in-vivo.mat\")\n",
    "\n",
    "    # numpy array\n",
    "    invivo_tensor = torch.from_numpy(invivo_data[\"m\"]).to(torch.float32)\n",
    "\n",
    "    # get prediction\n",
    "    network.to(\"cpu\")\n",
    "    pred_invivo = network(invivo_tensor)\n",
    "\n",
    "    # tensor back to numpy\n",
    "    pred_invivo_numpy = pred_invivo.detach().numpy()\n",
    "\n",
    "    # reshape into correct form\n",
    "    pred_invivo = np.reshape(pred_invivo_numpy, (128, 128, 60, 60), order = \"f\") # order f benötigt\n",
    "    \n",
    "    # load segmentation maps\n",
    "    seg_data = scipy.io.loadmat(r\"path/to/seg_data.mat\")\n",
    "    if seg_mask == \"brain_tissue\":\n",
    "        mask_data = scipy.io.loadmat(r\"path/to/mask.mat\")\n",
    "        mask = mask_data[\"mask\"]\n",
    "    elif seg_mask == \"wm\":\n",
    "        mask = seg_data[\"WM\"]\n",
    "    elif seg_mask == \"gm\":\n",
    "        mask = seg_data[\"GM\"]\n",
    "    elif seg_mask == \"csf\":\n",
    "        mask = seg_data[\"CSF\"]\n",
    "        \n",
    "    t1_data = np.linspace(50.0, 3500.0, num=60).reshape(1,-1)\n",
    "    t2_data = np.linspace(5.0, 400.0, num=60).reshape(1,-1)\n",
    "\n",
    "    colormap_cycle = ['Blues', 'Reds', 'Greens', 'Oranges', 'Purples', 'BuGn', 'YlOrBr', 'PuBu', 'PuRd', 'YlGNBu', 'Greys']\n",
    "\n",
    "    # apply mask to spectra\n",
    "    F_gmm_ = pred_invivo\n",
    "    nx = np.shape(F_gmm_)[0]\n",
    "    ny = np.shape(F_gmm_)[1]\n",
    "    q = np.shape(F_gmm_)[2]\n",
    "    mask = mask\n",
    "    F_gmm_ = np.multiply(F_gmm_, np.repeat(np.repeat(mask.reshape(np.shape(mask)[0], np.shape(mask)[1], 1, 1), q, axis=2), q, axis=3))\n",
    "\n",
    "    # threshold F_gmm_\n",
    "    F_gmm__thresh = F_gmm_.copy()\n",
    "    F_gmm__thresh[F_gmm__thresh < 1e-2] = 0\n",
    "\n",
    "    # convert spectra into counts\n",
    "    C_thresh = F_gmm__thresh * 1. / F_gmm__thresh[F_gmm__thresh > 0].min()\n",
    "    C_thresh = np.round(C_thresh)\n",
    "    C_thresh = C_thresh.astype(int)\n",
    "\n",
    "    C_thresh_mask = C_thresh[mask > 0]\n",
    "\n",
    "    C_thresh_mask_sum = np.sum(C_thresh_mask, axis=0)\n",
    "    C_thresh_mask_sum[C_thresh_mask_sum < 1000] = 0\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.contour(C_thresh_mask_sum, levels=60)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    # Put counts into T1-T2-list\n",
    "    t1_thresh = []\n",
    "    t2_thresh = []\n",
    "    for r in range(q):\n",
    "        for c in range(q):\n",
    "            t1_thresh = t1_thresh + [r] * C_thresh_mask_sum[r, c]  # [data['T1'][0][r]]*C_thresh_mask_sum[r, c]\n",
    "            t2_thresh = t2_thresh + [c] * C_thresh_mask_sum[r, c]  # [data['T2'][0][c]]*C_thresh_mask_sum[r, c]\n",
    "\n",
    "    # F_gmm_ormat for scatter\n",
    "    X = np.zeros([len(t1_thresh), 2])\n",
    "    X[:, 0] = t1_thresh\n",
    "    X[:, 1] = t2_thresh\n",
    "\n",
    "    # run GMM\n",
    "    gm = GaussianMixture(n_components=n_comp, covariance_type='full', verbose=True).fit(X)\n",
    "    labels = gm.predict(X)\n",
    "    labels = labels + 1  # to set only background to 0\n",
    "\n",
    "    # Go back to physical ranges\n",
    "    X_phys = X.copy()\n",
    "    for t1_ind, t1 in enumerate(t1_data[0]):\n",
    "        X_phys[X[:, 1] == t1_ind, 0] = t1\n",
    "    for t2_ind, t2 in enumerate(t2_data[0]):\n",
    "        X_phys[X[:, 0] == t2_ind, 1] = t2\n",
    "\n",
    "    # plot of all gaussians for average spectrum\n",
    "    examplary_voxels = 1000  # increasing this will make the plotting take longer\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    for i in range(1, n_comp + 1):\n",
    "        ind = np.where(labels == i)\n",
    "        ind = np.random.permutation(ind[0])\n",
    "        df = pd.DataFrame(data={'T1 (ms)': X_phys[ind, 0], 'T2 (ms)': X_phys[ind, 1]})\n",
    "        sns.kdeplot(df['T1 (ms)'], df['T2 (ms)'], shade=True, shade_lowest=False, alpha=0.6, antialiased=True, bw=1,\n",
    "                    cmap=colormap_cycle[i - 1])\n",
    "        plt.xlim(0, np.max(t1_data))\n",
    "        plt.ylim(0, np.max(t2_data))\n",
    "        plt.xlabel('T1 (ms)', size=14)\n",
    "        plt.ylabel('T2 (ms)', size=14)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # voxel wise labeling\n",
    "    label_array = np.zeros([nx, ny, n_comp])\n",
    "    label_array_mask = np.zeros([C_thresh_mask.shape[0], n_comp])\n",
    "\n",
    "    count_skip = 0\n",
    "\n",
    "    for i in range(C_thresh_mask.shape[0]):\n",
    "        t1_thresh_test = []\n",
    "        t2_thresh_test = []\n",
    "        try:\n",
    "            for r in range(q):\n",
    "                for c in range(q):\n",
    "                    t1_thresh_test = t1_thresh_test + [r] * C_thresh_mask[i, r, c]  # [data['T1'][0][r]]*C_thresh_mask[i,r, c]\n",
    "                    t2_thresh_test = t2_thresh_test + [c] * C_thresh_mask[i, r, c]  # [data['T2'][0][c]]*C_thresh_mask[i,r, c]\n",
    "\n",
    "            X_test = np.zeros([len(t1_thresh_test), 2])\n",
    "            X_test[:, 0] = t1_thresh_test\n",
    "            X_test[:, 1] = t2_thresh_test\n",
    "\n",
    "            labels_out = gm.predict(X_test)\n",
    "            counts_total = labels_out.shape[0]\n",
    "            for n in range(n_comp):\n",
    "                count = np.where(labels_out == n)[0]\n",
    "\n",
    "                try:\n",
    "                    label_array_mask[i, n] = count.shape[0] / counts_total\n",
    "                except ZeroDivisionError:\n",
    "                    label_array_mask[i, n] = 0.\n",
    "        except:\n",
    "            count_skip = count_skip + 1\n",
    "\n",
    "    label_array[mask > 0, :] = label_array_mask\n",
    "\n",
    "    # plot all maps besides\n",
    "    seg_pred = [] # liste mit segmentierungen der vorhersagen\n",
    "    x = n_comp  # len\n",
    "    y = 4       # width\n",
    "    for n in range(0, n_comp):   \n",
    "\n",
    "        fig = plt.figure(figsize=(16, 16)) \n",
    "\n",
    "        # compartmental volume fraction map\n",
    "        plt.subplot(x, y, 1).set_title(f'Compartment Nr.{n + 1}')\n",
    "        plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]) # deactivate xy ticks\n",
    "        plt.imshow(label_array[:, :, n], vmin=0, vmax=1)  \n",
    "        seg_pred.append(label_array[:, :, n])\n",
    "\n",
    "        # gaussian average spectrum\n",
    "        plt.subplot(x, y, 2).set_title(f'Compartment Nr.{n + 1}') \n",
    "        ind = np.where(labels == n + 1)\n",
    "        ind = np.random.permutation(ind[0])\n",
    "        df = pd.DataFrame(data={'T1 (ms)': X_phys[ind, 0], 'T2 (ms)': X_phys[ind, 1]})\n",
    "        sns.kdeplot(df['T1 (ms)'], df['T2 (ms)'], shade=True, shade_lowest=False, alpha=0.8, antialiased=True, bw=1,\n",
    "                    cmap=colormap_cycle[n - 1])\n",
    "        plt.xlim(0, np.max(t1_data))\n",
    "        plt.ylim(0, np.max(t2_data))\n",
    "        plt.xlabel('T1 (ms)', size=14)\n",
    "        plt.ylabel('T2 (ms)', size=14)\n",
    "\n",
    "        # histogram data over the threshhold\n",
    "        plt.subplot(x, y, 3).set_title(f'Compartment Nr.{n + 1}')\n",
    "        hist_list = list(filter(lambda x: x >= 0.1, label_array[:, :, n].reshape(-1))) # alle elemente unter einem threshhold aus der liste entfernen\n",
    "        plt.hist(hist_list, density=True, bins = 100) \n",
    "        mn, mx = plt.xlim()\n",
    "        plt.xlim(mn, mx)\n",
    "        kde_xs = np.linspace(mn, mx, 300)\n",
    "        kde = st.gaussian_kde(hist_list)\n",
    "        plt.plot(kde_xs, kde.pdf(kde_xs), label=\"PDF_gmm_\")\n",
    "        plt.ylabel('Vorkommen', size=14)\n",
    "        plt.xlabel('Wahrscheinlichkeit', size=14)\n",
    "        \n",
    "        # find out which compartment it is by getting the dice loss of each\n",
    "        seg_wert_GM = dice_loss(seg_pred[n], seg_data['GM'], smooth=1)\n",
    "        seg_wert_WM = dice_loss(seg_pred[n], seg_data['WM'], smooth=1)\n",
    "        seg_wert_CSF = dice_loss(seg_pred[n], seg_data['CSF'], smooth=1)\n",
    "        \n",
    "        print('der dice wert von gm, wm, csf: ', seg_wert_GM, seg_wert_WM, seg_wert_CSF)\n",
    "        \n",
    "        plt.subplot(x, y, 4).set_title(f'Compartment Nr.{n + 1}')\n",
    "        #plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]) # deactivate xy ticks\n",
    "        \n",
    "        if seg_wert_GM > seg_wert_WM and seg_wert_GM > seg_wert_CSF:\n",
    "            plt.imshow(seg_data['GM'] - seg_pred[n], vmin=0, vmax=1)\n",
    "        elif seg_wert_WM > seg_wert_GM and seg_wert_WM > seg_wert_CSF:\n",
    "            plt.imshow(seg_data['WM'] - seg_pred[n], vmin=0, vmax=1)\n",
    "        elif seg_wert_CSF > seg_wert_WM and seg_wert_CSF > seg_wert_GM:\n",
    "            plt.imshow(seg_data['CSF'] - seg_pred[n], vmin=0, vmax=1)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    print('---------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a9d1b-9d29-4667-a5ab-c2c16c98892c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "train_loader, test_loader, x_train, y_train, x_test, y_test, x_test_comp, y_test_comp = load_data()\n",
    "\n",
    "params = ['params_to_test'] \n",
    "\n",
    "# safe all trained networks for gmm analysis\n",
    "networks = []\n",
    "# safe all losses to plot them together\n",
    "train_losses_all = []\n",
    "test_losses_all = []\n",
    "\n",
    "for j in params:\n",
    "    \n",
    "    param_to_test = j\n",
    "    \n",
    "    print(\"Parameter: \", str(j))\n",
    "    \n",
    "    # training\n",
    "    network, optimizer = preprocess()\n",
    "\n",
    "    # Define variable that should save the loss for each iteration\n",
    "    train_loss = []\n",
    "    train_losses = []\n",
    "    test_loss = []\n",
    "    test_losses = []\n",
    "\n",
    "    # do the actual training\n",
    "    network = train_test(network, optimizer)\n",
    "\n",
    "    train_losses_all.append(train_losses)\n",
    "    test_losses_all.append(test_losses)\n",
    "\n",
    "    # loss Eval\n",
    "    epochen = [j  for j in range(len(train_losses))]\n",
    "    plot_train_test_loss(epochen, train_losses, test_losses) # plot both\n",
    "    \n",
    "    # comp losses\n",
    "    for k in range(0, 4):\n",
    "        comp_loss = test_comp(network, x_test_comp[k], y_test_comp[k])\n",
    "        print('loss bei ', str(k + 1), 'compartments: ', comp_loss)\n",
    "    \n",
    "    # plot spectra prediction\n",
    "    if use_plot == True:\n",
    "        network = network.cpu()\n",
    "        plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "        print(\"train\")\n",
    "        for m in range(0, 2):\n",
    "            plot_3d_spect_vergleich(y_train[m].cpu(), predict(network, x_train[m]).detach().numpy())\n",
    "        print(\"test\")\n",
    "        for i in range(0, 2):\n",
    "            plot_3d_spect_vergleich(y_test[i].cpu(), predict(network, x_test[i]).detach().numpy())\n",
    "            \n",
    "    networks.append(network)\n",
    "\n",
    "    # save modell \n",
    "    Path = \"model_trained_with_\" + str(j)\n",
    "    torch.save(network, Path)     \n",
    "    \n",
    "    # clear cache for next cycle \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2adff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot train loss of all tests\n",
    "fig = plt.figure()\n",
    "for i in range(0, len(train_losses_all)):\n",
    "    plt.plot([j for j in range(len(train_losses_all[i]))], train_losses_all[i], label='parameter: '+str(params[i]))\n",
    "    \n",
    "plt.legend(['parameter '+str(params[h]) for h in range(0, len(train_losses_all))], loc='upper right')\n",
    "\n",
    "# plot test loss of all tests\n",
    "fig = plt.figure()\n",
    "for i in range(0, len(test_losses_all)):\n",
    "    plt.plot([j for j in range(len(test_losses_all[i]))], test_losses_all[i], label='parameter: '+str(params[i]))\n",
    "    \n",
    "plt.legend(['parameter '+str(params[h]) for h in range(0, len(test_losses_all))], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffaa2b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# gmm eval\n",
    "for n in range(0, len(params)):\n",
    "    print(params[n])\n",
    "    gmm_eval(networks[n], 'brain_tissue', n_comp=4)\n",
    "    \n",
    "    print('einzelne GMM Analysen')\n",
    "    \n",
    "    print('White Matter Analysis for Parameter', params[n])\n",
    "    gmm_eval(networks[n], 'wm', n_comp=3)\n",
    "    print('Gray Matter Analysis for Parameter', params[n])\n",
    "    gmm_eval(networks[n], 'gm', n_comp=3)\n",
    "    print('CSF Analysis for Parameter', params[n])\n",
    "    gmm_eval(networks[n], 'csf', n_comp=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
